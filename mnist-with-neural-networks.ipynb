{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport tensorflow as tf\nfrom tensorflow.keras.datasets import mnist\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-09-01T09:05:09.498876Z","iopub.execute_input":"2024-09-01T09:05:09.499343Z","iopub.status.idle":"2024-09-01T09:05:25.622356Z","shell.execute_reply.started":"2024-09-01T09:05:09.499296Z","shell.execute_reply":"2024-09-01T09:05:25.621072Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Framework\n\nclass Linear:\n    def __init__(self, input_dim, output_dim):\n        self.weights = np.random.randn(input_dim, output_dim) * 0.01\n        self.biases = np.zeros((1, output_dim))\n    \n    def forward(self, x):\n        self.input = x\n        return np.dot(x, self.weights) + self.biases\n    \n    def backward(self, grad_output):\n        grad_input = np.dot(grad_output, self.weights.T)\n        self.grad_weights = np.dot(self.input.T, grad_output)\n        self.grad_biases = np.sum(grad_output, axis=0, keepdims=True)\n        return grad_input\n\nclass ReLU:\n    def forward(self, x):\n        self.input = x\n        return np.maximum(0, x)\n    \n    def backward(self, grad_output):\n        return grad_output * (self.input > 0)\n\nclass Sigmoid:\n    def forward(self, x):\n        self.output = 1 / (1 + np.exp(-x))\n        return self.output\n    \n    def backward(self, grad_output):\n        return grad_output * (1 - self.output) * self.output\n\nclass Tanh:\n    def forward(self, x):\n        self.output = np.tanh(x)\n        return self.output\n    \n    def backward(self, grad_output):\n        return grad_output * self.output\n\nclass Softmax:\n    def forward(self, x):\n        exp_vals = np.exp(x - np.max(x, axis=1, keepdims=True))\n        self.output = exp_vals / np.sum(exp_vals, axis=1, keepdims=True)\n        return self.output\n    \n    def backward(self, grad_output):\n        return grad_output\n\nclass CrossEntropyLoss:\n    def forward(self, predictions, targets):\n        self.predictions = predictions\n        self.targets = targets\n        return -np.mean(np.sum(targets * np.log(predictions + 1e-9), axis=1))\n    \n    def backward(self):\n        return self.predictions - self.targets\n\nclass SGD:\n    def __init__(self, learning_rate=0.01):\n        self.learning_rate = learning_rate\n    \n    def step(self, layers):\n        for layer in layers:\n            if hasattr(layer, 'weights'):\n                layer.weights -= self.learning_rate * layer.grad_weights\n                layer.biases -= self.learning_rate * layer.grad_biases\n\nclass Model:\n    def __init__(self):\n        self.layers = []\n    \n    def add_layer(self, layer):\n        self.layers.append(layer)\n    \n    def compile(self, loss, optimizer):\n        self.loss = loss\n        self.optimizer = optimizer\n    \n    def forward(self, x):\n        for layer in self.layers:\n            x = layer.forward(x)\n        return x\n    \n    def backward(self, grad_output):\n        for layer in reversed(self.layers):\n            grad_output = layer.backward(grad_output)\n    \n    def train(self, x_train, y_train, epochs, batch_size):\n        num_samples = x_train.shape[0]\n        for epoch in range(epochs):\n            for i in range(0, num_samples, batch_size):\n                x_batch = x_train[i:i+batch_size]\n                y_batch = y_train[i:i+batch_size]\n                \n                predictions = self.forward(x_batch)\n                loss = self.loss.forward(predictions, y_batch)\n                grad_output = self.loss.backward()\n                self.backward(grad_output)\n                self.optimizer.step(self.layers)\n            \n            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss}')\n    \n    def evaluate(self, x_test, y_test):\n        predictions = self.forward(x_test)\n        loss = self.loss.forward(predictions, y_test)\n        accuracy = np.mean(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1))\n        return loss, accuracy\n    \n    def save(self, path):\n        np.savez(path, layers=self.layers)\n    \n    def load(self, path):\n        data = np.load(path, allow_pickle=True)\n        self.layers = data['layers']\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train = x_train.reshape(-1, 28*28) / 255.0\nx_test = x_test.reshape(-1, 28*28) / 255.0\ny_train = tf.keras.utils.to_categorical(y_train, 10)\ny_test = tf.keras.utils.to_categorical(y_test, 10)\n\nmodel = Model()\nmodel.add_layer(Linear(784, 128))\nmodel.add_layer(ReLU())\nmodel.add_layer(Linear(128, 10))\nmodel.add_layer(Softmax())\n\nloss = CrossEntropyLoss()\noptimizer = SGD(learning_rate=0.01)\nmodel.compile(loss, optimizer)\n\nmodel.train(x_train, y_train, epochs=20, batch_size=64)\n\ntest_loss, test_accuracy = model.evaluate(x_test, y_test)\nprint(f'Test Loss: {test_loss}, Test Accuracy: {test_accuracy}')\n","metadata":{"execution":{"iopub.status.busy":"2024-09-01T09:05:32.886926Z","iopub.execute_input":"2024-09-01T09:05:32.887785Z","iopub.status.idle":"2024-09-01T09:06:21.202008Z","shell.execute_reply.started":"2024-09-01T09:05:32.887727Z","shell.execute_reply":"2024-09-01T09:06:21.200830Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\nEpoch 1/20, Loss: 0.023553245909162957\nEpoch 2/20, Loss: 0.01171022812807396\nEpoch 3/20, Loss: 0.011626898816245615\nEpoch 4/20, Loss: 0.008513983205010388\nEpoch 5/20, Loss: 0.005005812198133689\nEpoch 6/20, Loss: 0.004006683536088857\nEpoch 7/20, Loss: 0.002571963199886822\nEpoch 8/20, Loss: 0.03943089540350052\nEpoch 9/20, Loss: 0.010950708120187828\nEpoch 10/20, Loss: 0.002560191559125\nEpoch 11/20, Loss: 0.0024847129190278517\nEpoch 12/20, Loss: 0.006422046552829688\nEpoch 13/20, Loss: 0.0032314175015804207\nEpoch 14/20, Loss: 0.0020074050572785078\nEpoch 15/20, Loss: 0.00045350506348711367\nEpoch 16/20, Loss: 0.000421626217944856\nEpoch 17/20, Loss: 0.00046683132808186757\nEpoch 18/20, Loss: 0.0002629850256379771\nEpoch 19/20, Loss: 0.00019633501537519074\nEpoch 20/20, Loss: 0.0001998771112928467\nTest Loss: 0.09978779383016965, Test Accuracy: 0.9783\n","output_type":"stream"}]}]}